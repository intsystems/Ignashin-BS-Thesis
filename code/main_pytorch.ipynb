{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd code    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import (TransformerEncoder, TransformerDecoder,\n",
    "                      TransformerEncoderLayer, TransformerDecoderLayer)\n",
    "\n",
    "from src.prepare_data import download_data, build_train_vocab, get_train_test_val, check_tokens,tokens_to_sentence , generate_batch , visualize_iter_data , get_embed\n",
    "# from src.LSTM import RNNdecoder, RNNencoder,Seq2SeqRNN\n",
    "from src.train import create_mask,generate_square_subsequent_mask, train_epoch, bleu_calculate , evaluate\n",
    "# from src.transformer import Seq2SeqTransformer,PositionalEncoding,TokenEmbedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.LSTM_pytorch as srcLSTM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "De vocab En vocab:  19215 10838\n",
      "Train Test Val:  29000 1014 1000\n"
     ]
    }
   ],
   "source": [
    "train_filepaths , val_filepaths , test_filepaths = download_data()\n",
    "de_vocab, en_vocab, de_tokenizer, en_tokenizer = build_train_vocab(train_filepaths)\n",
    "print( 'De vocab En vocab: ',len(de_vocab), len(en_vocab))\n",
    "train_data , val_data , test_data = get_train_test_val(train_filepaths, test_filepaths, val_filepaths , de_vocab , en_vocab ,de_tokenizer,en_tokenizer )\n",
    "print('Train Test Val: ',len(train_data),len(test_data) , len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "PAD_IDX = de_vocab['<pad>']\n",
    "BOS_IDX = de_vocab['<bos>']\n",
    "EOS_IDX = de_vocab['<eos>']\n",
    "print(PAD_IDX , BOS_IDX , EOS_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prepareData' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m hidden_size \u001b[39m=\u001b[39m \u001b[39m128\u001b[39m\n\u001b[1;32m      2\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m32\u001b[39m\n\u001b[0;32m----> 4\u001b[0m input_lang, output_lang, train_dataloader \u001b[39m=\u001b[39m srcLSTM\u001b[39m.\u001b[39;49mget_dataloader(batch_size)\n\u001b[1;32m      6\u001b[0m encoder \u001b[39m=\u001b[39m srcLSTM\u001b[39m.\u001b[39mEncoderRNN(input_lang\u001b[39m.\u001b[39mn_words, hidden_size)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m decoder \u001b[39m=\u001b[39m srcLSTM\u001b[39m.\u001b[39mAttnDecoderRNN(hidden_size, output_lang\u001b[39m.\u001b[39mn_words)\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/Desktop/DIPLOM/Ignashin-BS-Thesis/code/src/LSTM_pytorch.py:141\u001b[0m, in \u001b[0;36mget_dataloader\u001b[0;34m(batch_size)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_dataloader\u001b[39m(batch_size):\n\u001b[0;32m--> 141\u001b[0m     input_lang, output_lang, pairs \u001b[39m=\u001b[39m prepareData(\u001b[39m'\u001b[39m\u001b[39meng\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfra\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    143\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(pairs)\n\u001b[1;32m    144\u001b[0m     input_ids \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((n, MAX_LENGTH), dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mint32)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prepareData' is not defined"
     ]
    }
   ],
   "source": [
    "hidden_size = 128\n",
    "batch_size = 32\n",
    "\n",
    "input_lang, output_lang, train_dataloader = srcLSTM.get_dataloader(batch_size)\n",
    "\n",
    "encoder = srcLSTM.EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder = srcLSTM.AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
    "\n",
    "srcLSTM.train(train_dataloader, encoder, decoder, 80, print_every=5, plot_every=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
