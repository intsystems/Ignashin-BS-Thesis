{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualizzation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SRC.vocab.itos[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRG.vocab.itos[::1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/content/drive/MyDrive/DIPLOM/data/fields.pkl', 'wb') as f:\n",
    "#     torch.save((SRC, TRG), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/content/drive/MyDrive/DIPLOM/data/fields.pkl', 'rb') as f:\n",
    "#     SRC, TRG = torch.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Unique tokens in source (ru) vocabulary: {len(SRC.vocab)}\")\n",
    "# print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(vars(train_data.examples[9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src_length = map(len, [vars(x)['src'] for x in train_data.examples])\n",
    "# trg_length = map(len, [vars(x)['trg'] for x in train_data.examples])\n",
    "\n",
    "# print('Length distribution in Train data')\n",
    "# plt.figure(figsize=[8, 4])\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.title(\"source length\")\n",
    "# plt.hist(list(src_length), bins=20);\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.title(\"translation length\")\n",
    "# plt.hist(list(trg_length), bins=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src_length = map(len, [vars(x)['src'] for x in test_data.examples])\n",
    "# trg_length = map(len, [vars(x)['trg'] for x in test_data.examples])\n",
    "\n",
    "# print('Length distribution in Test data')\n",
    "# plt.figure(figsize=[8, 4])\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.title(\"source length\")\n",
    "# plt.hist(list(src_length), bins=20);\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.title(\"translation length\")\n",
    "# plt.hist(list(trg_length), bins=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT_DIM = len(SRC.vocab)\n",
    "# OUTPUT_DIM = len(TRG.vocab)\n",
    "# ENC_EMB_DIM = 512\n",
    "# DEC_EMB_DIM = 512\n",
    "# HID_DIM = 512\n",
    "# N_LAYERS = 1\n",
    "# ENC_DROPOUT = 0.5\n",
    "# DEC_DROPOUT = 0.5\n",
    "\n",
    "# enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "# dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "# # dont forget to put the model to the right device\n",
    "# model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def init_weights(m):\n",
    "#     for name, param in m.named_parameters():\n",
    "#         nn.init.uniform_(param, -0.08, 0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def count_parameters(model):\n",
    "#     return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PAD_IDX = TRG.vocab.stoi['<pad>']\n",
    "# optimizer = optim.Adam(model.parameters())\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_history = []\n",
    "# valid_history = []\n",
    "\n",
    "# N_EPOCHS = 10\n",
    "# CLIP = 1\n",
    "\n",
    "# best_valid_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(N_EPOCHS):\n",
    "\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     train_loss = train(model, train_iterator, optimizer, criterion, CLIP, train_history, valid_history)\n",
    "#     valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "#     end_time = time.time()\n",
    "\n",
    "#     epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "#     if valid_loss < best_valid_loss:\n",
    "#         best_valid_loss = valid_loss\n",
    "#         torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "\n",
    "#     train_history.append(train_loss)\n",
    "#     valid_history.append(valid_loss)\n",
    "#     print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "#     print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "#     print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST MODELS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- import utils\n",
    "import imp\n",
    "imp.reload(utils)\n",
    "generate_translation = utils.generate_translation\n",
    "remove_tech_tokens = utils.remove_tech_tokens\n",
    "get_text = utils.get_text\n",
    "flatten = utils.flatten -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import utils\n",
    "# import imp\n",
    "# imp.reload(utils)\n",
    "# generate_translation = utils.generate_translation\n",
    "# remove_tech_tokens = utils.remove_tech_tokens\n",
    "# get_text = utils.get_text\n",
    "# flatten = utils.flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
