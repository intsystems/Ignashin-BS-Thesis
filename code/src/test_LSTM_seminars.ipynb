{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import (TransformerEncoder, TransformerDecoder,\n",
    "                      TransformerEncoderLayer, TransformerDecoderLayer)\n",
    "\n",
    "from src.prepare_data import download_data, build_train_vocab, get_train_test_val, check_tokens,tokens_to_sentence , generate_batch , visualize_iter_data , get_embed\n",
    "from src.LSTM import RNNdecoder, RNNencoder,Seq2SeqRNN\n",
    "from src.train import create_mask,generate_square_subsequent_mask, train_epoch, bleu_calculate , evaluate\n",
    "from src.transformer import Seq2SeqTransformer,PositionalEncoding,TokenEmbedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.LSTM_seminars import seq2seq\n",
    "from src.train_seminars import trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "De vocab En vocab:  19215 10838\n",
      "Train Test Val:  29000 1014 1000\n"
     ]
    }
   ],
   "source": [
    "train_filepaths , val_filepaths , test_filepaths = download_data()\n",
    "de_vocab, en_vocab, de_tokenizer, en_tokenizer = build_train_vocab(train_filepaths)\n",
    "print( 'De vocab En vocab: ',len(de_vocab), len(en_vocab))\n",
    "train_data , val_data , test_data = get_train_test_val(train_filepaths, test_filepaths, val_filepaths , de_vocab , en_vocab ,de_tokenizer,en_tokenizer )\n",
    "print('Train Test Val: ',len(train_data),len(test_data) , len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "PAD_IDX = de_vocab['<pad>']\n",
    "BOS_IDX = de_vocab['<bos>']\n",
    "EOS_IDX = de_vocab['<eos>']\n",
    "print(PAD_IDX , BOS_IDX , EOS_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = DataLoader(train_data, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, collate_fn= lambda x : generate_batch(x , BOS_IDX=BOS_IDX,PAD_IDX=PAD_IDX,EOS_IDX=EOS_IDX))\n",
    "valid_iter = DataLoader(val_data, batch_size=1,\n",
    "                        shuffle=True, collate_fn= lambda x : generate_batch(x , BOS_IDX=BOS_IDX,PAD_IDX=PAD_IDX,EOS_IDX=EOS_IDX))\n",
    "test_iter = DataLoader(test_data, batch_size=BATCH_SIZE,\n",
    "                       shuffle=True, collate_fn= lambda x : generate_batch(x , BOS_IDX=BOS_IDX,PAD_IDX=PAD_IDX,EOS_IDX=EOS_IDX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_VOCAB_SIZE = len(de_vocab)\n",
    "TGT_VOCAB_SIZE = len(en_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "seq2seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(19215, 30)\n",
       "    (lstm): LSTM(30, 30, batch_first=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(19215, 30)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=30, out_features=30, bias=True)\n",
       "    )\n",
       "    (lstm): LSTM(30, 30, batch_first=True)\n",
       "    (linear): Linear(in_features=30, out_features=19215, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = seq2seq(SRC_VOCAB_SIZE)\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = torch.optim.Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_train_dataset = []\n",
    "en_train_dataset = []\n",
    "for j in train_data :\n",
    "    de_train_dataset.append(j[0].tolist())\n",
    "    en_train_dataset.append(j[1].tolist())\n",
    "de_train_dataset = np.array(de_train_dataset, dtype=object)\n",
    "en_train_dataset = np.array(en_train_dataset, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(dataset=train_data, \n",
    "shuffle=True, pin_memory=True, batch_size=BATCH_SIZE,collate_fn= lambda x : generate_batch(x , BOS_IDX=BOS_IDX,PAD_IDX=PAD_IDX,EOS_IDX=EOS_IDX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:   0%|          | 0/15 [00:00<?, ?it/s, train epoch loss=nan]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer(count_of_epoch\u001b[39m=\u001b[39;49m\u001b[39m15\u001b[39;49m, \n\u001b[1;32m      2\u001b[0m         batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m, \n\u001b[1;32m      3\u001b[0m         dataloader\u001b[39m=\u001b[39;49mtrain_dataloader,\n\u001b[1;32m      4\u001b[0m         model\u001b[39m=\u001b[39;49mmodel, \n\u001b[1;32m      5\u001b[0m         loss_function\u001b[39m=\u001b[39;49mloss_function,\n\u001b[1;32m      6\u001b[0m         optimizer \u001b[39m=\u001b[39;49m optimizer,\n\u001b[1;32m      7\u001b[0m         lr\u001b[39m=\u001b[39;49m\u001b[39m0.001\u001b[39;49m,\n\u001b[1;32m      8\u001b[0m         callback\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/Desktop/DIPLOM/Ignashin-BS-Thesis/code/src/train_seminars.py:52\u001b[0m, in \u001b[0;36mtrainer\u001b[0;34m(count_of_epoch, batch_size, dataloader, model, loss_function, optimizer, lr, callback)\u001b[0m\n\u001b[1;32m     48\u001b[0m iterations\u001b[39m.\u001b[39mset_postfix({\u001b[39m'\u001b[39m\u001b[39mtrain epoch loss\u001b[39m\u001b[39m'\u001b[39m: np\u001b[39m.\u001b[39mnan})\n\u001b[1;32m     49\u001b[0m \u001b[39mfor\u001b[39;00m it \u001b[39min\u001b[39;00m iterations:\n\u001b[1;32m     50\u001b[0m     batch_generator \u001b[39m=\u001b[39m tqdm(\n\u001b[1;32m     51\u001b[0m         dataloader, \n\u001b[0;32m---> 52\u001b[0m         leave\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(dataset)\u001b[39m/\u001b[39m\u001b[39m/\u001b[39mbatch_size\u001b[39m+\u001b[39m(\u001b[39mlen\u001b[39m(dataset)\u001b[39m%\u001b[39mbatch_size\u001b[39m>\u001b[39m\u001b[39m0\u001b[39m))\n\u001b[1;32m     54\u001b[0m     epoch_loss \u001b[39m=\u001b[39m train_epoch(train_generator\u001b[39m=\u001b[39mbatch_generator, \n\u001b[1;32m     55\u001b[0m                 model\u001b[39m=\u001b[39mmodel, \n\u001b[1;32m     56\u001b[0m                 loss_function\u001b[39m=\u001b[39mloss_function, \n\u001b[1;32m     57\u001b[0m                 optimizer\u001b[39m=\u001b[39moptima, \n\u001b[1;32m     58\u001b[0m                 callback\u001b[39m=\u001b[39mcallback)\n\u001b[1;32m     60\u001b[0m     iterations\u001b[39m.\u001b[39mset_postfix({\u001b[39m'\u001b[39m\u001b[39mtrain epoch loss\u001b[39m\u001b[39m'\u001b[39m: epoch_loss})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "trainer(count_of_epoch=15, \n",
    "        batch_size=64, \n",
    "        dataloader=train_dataloader,\n",
    "        dataset = tran_data ,\n",
    "        model=model, \n",
    "        loss_function=loss_function,\n",
    "        optimizer = optimizer,\n",
    "        lr=0.001,\n",
    "        callback=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([24, 25, 26, 27, 11, 22, 28, 29, 30, 16]),\n",
       " tensor([26, 27, 28, 29, 30, 22, 31, 32, 14]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
